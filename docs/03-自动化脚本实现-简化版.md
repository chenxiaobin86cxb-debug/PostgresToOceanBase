# PostgreSQL 到 OceanBase 迁移脚本实现（简化版）

## 一、技术栈选择

### 1.1 语言和框架

**选择：Python 3.8+**

**理由**：
- 丰富的数据库驱动支持
- 易于处理数据类型转换
- 良好的异步支持（asyncio）
- 便于部署和扩展

### 1.2 核心依赖库

```txt
requirements.txt
```

```txt
# PostgreSQL 驱动
psycopg2-binary==2.9.9

# OceanBase MySQL 驱动
pymysql==1.1.0

# 配置管理
pyyaml==6.0.1
python-dotenv==1.0.0

# 进度和日志
tqdm==4.66.1
loguru==0.7.2

# 工具库
pandas==2.1.4
numpy==1.26.2

# 类型转换
pydantic==2.5.2
```

### 1.3 项目结构

```
PostgresToOceanBase/
├── config/
│   ├── config.yaml          # 主配置文件
│   ├── type_mapping.yaml     # 数据类型映射配置
│   └── logger.yaml          # 日志配置
├── src/
│   ├── __init__.py
│   ├── database/
│   │   ├── __init__.py
│   │   ├── postgres.py      # PostgreSQL 连接和操作
│   │   ├── oceanbase.py     # OceanBase 连接和操作
│   │   └── connection.py    # 连接池管理
│   ├── migration/
│   │   ├── __init__.py
│   │   ├── schema.py        # 表结构迁移
│   │   ├── data.py         # 数据迁移
│   │   ├── validator.py    # 数据验证
│   │   └── converter.py    # 数据类型转换
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── logger.py       # 日志工具
│   │   ├── progress.py     # 进度显示
│   │   └── retry.py        # 重试机制
│   └── main.py            # 主程序入口
├── tests/
│   ├── test_schema.py       # 表结构迁移测试
│   ├── test_data.py        # 数据迁移测试
│   └── test_converter.py   # 类型转换测试
├── logs/                   # 日志目录
├── backup/                 # 备份目录
├── .env                    # 环境变量
├── .gitignore
├── requirements.txt
├── setup.py                # 安装脚本
└── README.md
```

---

## 二、核心实现代码

### 2.1 配置文件

#### `config/config.yaml`

```yaml
# PostgreSQL 源端配置
source:
  host: localhost
  port: 5432
  database: mydb
  user: migration_user
  password: ${POSTGRES_PASSWORD}
  schema: public
  pool_size: 10
  max_overflow: 20

# OceanBase 目标端配置（MySQL 租户）
target:
  host: localhost
  port: 2881
  database: mydb
  user: migration_user@tenant
  password: ${OCEANBASE_PASSWORD}
  mode: mysql
  pool_size: 10
  max_overflow: 20

# 迁移配置
migration:
  # 表结构迁移
  schema:
    enabled: true
    ignore_types: ['json', 'jsonb', 'array']  # 忽略的数据类型
    include_tables: []    # 空表示所有表，否则指定表名列表
    exclude_tables: []    # 排除的表名列表

  # 数据迁移
  data:
    enabled: true
    batch_size: 1000      # 批量插入大小
    parallel_workers: 4   # 并发工作数
    chunk_size: 10000     # 每次读取的记录数
    progress_interval: 10 # 进度显示间隔（秒）

  # 数据验证
  validation:
    enabled: true
    sample_size: 1000     # 抽样验证数量
    check_count: true      # 检查记录数
    check_checksum: true   # 检查校验和

  # 错误处理
  error:
    max_retries: 3        # 最大重试次数
    retry_delay: 5        # 重试延迟（秒）
    continue_on_error: false  # 出错后是否继续

# 日志配置
logging:
  level: INFO
  file: logs/migration.log
  rotation: 100 MB
  retention: 30 days
```

#### `config/type_mapping.yaml`

```yaml
# PostgreSQL 到 OceanBase MySQL 租户类型映射
postgres_to_mysql:
  smallint: SMALLINT
  integer: INT
  bigint: BIGINT
  serial: INT AUTO_INCREMENT
  bigserial: BIGINT AUTO_INCREMENT
  decimal: DECIMAL({precision},{scale})
  numeric: DECIMAL({precision},{scale})
  real: FLOAT
  double precision: DOUBLE
  boolean: TINYINT(1)
  char: CHAR({length})
  varchar: VARCHAR({length})
  text: TEXT
  bytea: VARBINARY(65535)
  date: DATE
  timestamp: TIMESTAMP
  timestamptz: TIMESTAMP
  uuid: VARCHAR(36)
  enum: VARCHAR(255)
```

### 2.2 数据库连接模块

#### `src/database/connection.py`

```python
import logging
from typing import Optional
from contextlib import contextmanager
import psycopg2
import pymysql
from psycopg2 import pool as pg_pool
from pymysql import pool as mysql_pool
from loguru import logger


class ConnectionManager:
    """数据库连接管理器"""

    def __init__(self, config: dict):
        self.source_config = config.get('source', {})
        self.target_config = config.get('target', {})
        self.source_pool: Optional[pg_pool.ThreadedConnectionPool] = None
        self.target_pool: Optional[mysql_pool.PooledDB] = None

    def init_pools(self):
        """初始化连接池"""
        # PostgreSQL 连接池
        try:
            self.source_pool = pg_pool.ThreadedConnectionPool(
                minconn=1,
                maxconn=self.source_config.get('pool_size', 10) + 
                         self.source_config.get('max_overflow', 20),
                host=self.source_config.get('host'),
                port=self.source_config.get('port'),
                database=self.source_config.get('database'),
                user=self.source_config.get('user'),
                password=self.source_config.get('password')
            )
            logger.info("PostgreSQL 连接池初始化成功")
        except Exception as e:
            logger.error(f"PostgreSQL 连接池初始化失败: {e}")
            raise

        # OceanBase 连接池（MySQL 模式）
        try:
            self.target_pool = mysql_pool.PooledDB(
                creator=pymysql,
                maxconnections=self.target_config.get('pool_size', 10) +
                             self.target_config.get('max_overflow', 20),
                host=self.target_config.get('host'),
                port=self.target_config.get('port'),
                database=self.target_config.get('database'),
                user=self.target_config.get('user'),
                password=self.target_config.get('password'),
                charset='utf8mb4',
                autocommit=False
            )
            logger.info("OceanBase 连接池初始化成功")
        except Exception as e:
            logger.error(f"OceanBase 连接池初始化失败: {e}")
            raise

    @contextmanager
    def get_source_connection(self):
        """获取 PostgreSQL 连接"""
        conn = None
        try:
            conn = self.source_pool.getconn()
            yield conn
        except Exception as e:
            logger.error(f"PostgreSQL 连接错误: {e}")
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                self.source_pool.putconn(conn)

    @contextmanager
    def get_target_connection(self):
        """获取 OceanBase 连接"""
        conn = None
        try:
            conn = self.target_pool.connection()
            yield conn
        except Exception as e:
            logger.error(f"OceanBase 连接错误: {e}")
            if conn:
                conn.rollback()
            raise
        finally:
            if conn:
                conn.close()

    def close_all(self):
        """关闭所有连接池"""
        if self.source_pool:
            self.source_pool.closeall()
            logger.info("PostgreSQL 连接池已关闭")
        if self.target_pool:
            self.target_pool.close()
            logger.info("OceanBase 连接池已关闭")
```

#### `src/database/postgres.py`

```python
from typing import List, Dict, Any, Optional
from loguru import logger


class PostgreSQLClient:
    """PostgreSQL 客户端"""

    def __init__(self, connection_manager):
        self.conn_mgr = connection_manager

    def get_tables(self, schema: str = 'public') -> List[str]:
        """获取所有表名"""
        with self.conn_mgr.get_source_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT tablename 
                FROM pg_tables 
                WHERE schemaname = %s
                ORDER BY tablename
            """, (schema,))
            tables = [row[0] for row in cursor.fetchall()]
            cursor.close()
            return tables

    def get_table_schema(self, table_name: str, schema: str = 'public') -> Dict[str, Any]:
        """获取表结构"""
        with self.conn_mgr.get_source_connection() as conn:
            cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            
            # 获取列信息
            cursor.execute("""
                SELECT 
                    column_name,
                    data_type,
                    udt_name,
                    character_maximum_length,
                    numeric_precision,
                    numeric_scale,
                    is_nullable,
                    column_default
                FROM information_schema.columns
                WHERE table_schema = %s AND table_name = %s
                ORDER BY ordinal_position
            """, (schema, table_name))
            columns = [dict(row) for row in cursor.fetchall()]
            
            # 获取主键信息
            cursor.execute("""
                SELECT a.attname
                FROM pg_index i
                JOIN pg_attribute a ON a.attrelid = i.indrelid AND a.attnum = ANY(i.indkey)
                WHERE i.indrelid = %s::regclass AND i.indisprimary
            """, (f"{schema}.{table_name}",))
            primary_keys = [row[0] for row in cursor.fetchall()]
            
            cursor.close()
            return {
                'table_name': table_name,
                'columns': columns,
                'primary_keys': primary_keys
            }

    def get_table_count(self, table_name: str, schema: str = 'public') -> int:
        """获取表记录数"""
        with self.conn_mgr.get_source_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(f"SELECT COUNT(*) FROM {schema}.{table_name}")
            count = cursor.fetchone()[0]
            cursor.close()
            return count

    def get_table_data(self, table_name: str, schema: str = 'public', 
                     offset: int = 0, limit: int = 1000,
                     exclude_columns: List[str] = None) -> List[Dict]:
        """获取表数据"""
        if exclude_columns is None:
            exclude_columns = []
        
        with self.conn_mgr.get_source_connection() as conn:
            cursor = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)
            
            # 构建 SELECT 语句，排除特定字段
            if exclude_columns:
                # 获取所有列名
                cursor.execute("""
                    SELECT column_name 
                    FROM information_schema.columns
                    WHERE table_schema = %s AND table_name = %s
                    ORDER BY ordinal_position
                """, (schema, table_name))
                all_columns = [row[0] for row in cursor.fetchall()]
                # 过滤排除的列
                select_columns = [col for col in all_columns if col not in exclude_columns]
                columns_str = ', '.join(select_columns)
            else:
                columns_str = '*'
            
            cursor.execute(f"""
                SELECT {columns_str} FROM {schema}.{table_name}
                ORDER BY (SELECT NULL)
                OFFSET %s LIMIT %s
            """, (offset, limit))
            rows = [dict(row) for row in cursor.fetchall()]
            cursor.close()
            return rows
```

#### `src/database/oceanbase.py`

```python
from typing import List, Dict, Any
from loguru import logger


class OceanBaseClient:
    """OceanBase MySQL 客户端"""

    def __init__(self, connection_manager):
        self.conn_mgr = connection_manager

    def create_table(self, schema_sql: str) -> bool:
        """创建表"""
        with self.conn_mgr.get_target_connection() as conn:
            cursor = conn.cursor()
            try:
                cursor.execute(schema_sql)
                conn.commit()
                logger.info(f"表创建成功: {schema_sql[:50]}...")
                return True
            except Exception as e:
                conn.rollback()
                logger.error(f"表创建失败: {e}")
                return False
            finally:
                cursor.close()

    def insert_batch(self, table_name: str, data: List[Dict], 
                   batch_size: int = 1000) -> int:
        """批量插入数据"""
        if not data:
            return 0

        with self.conn_mgr.get_target_connection() as conn:
            cursor = conn.cursor()
            inserted = 0
            try:
                # 分批插入
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    columns = list(batch[0].keys())
                    placeholders = ', '.join(['%s'] * len(columns))
                    columns_str = ', '.join(columns)
                    
                    values = [[row[col] for col in columns] for row in batch]
                    
                    cursor.executemany(
                        f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})",
                        values
                    )
                    conn.commit()
                    inserted += len(batch)
                
                logger.info(f"批量插入完成: {table_name}, 共 {inserted} 条")
                return inserted
            except Exception as e:
                conn.rollback()
                logger.error(f"批量插入失败: {table_name}, 错误: {e}")
                return 0
            finally:
                cursor.close()

    def get_table_count(self, table_name: str) -> int:
        """获取表记录数"""
        with self.conn_mgr.get_target_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            cursor.close()
            return count

    def truncate_table(self, table_name: str) -> bool:
        """清空表"""
        with self.conn_mgr.get_target_connection() as conn:
            cursor = conn.cursor()
            try:
                cursor.execute(f"TRUNCATE TABLE {table_name}")
                conn.commit()
                logger.info(f"表清空成功: {table_name}")
                return True
            except Exception as e:
                conn.rollback()
                logger.error(f"表清空失败: {table_name}, 错误: {e}")
                return False
            finally:
                cursor.close()
```

### 2.3 数据类型转换模块

#### `src/migration/converter.py`

```python
import yaml
from typing import Dict, Any
from loguru import logger


class TypeConverter:
    """数据类型转换器"""

    def __init__(self, mapping_file: str = 'config/type_mapping.yaml'):
        with open(mapping_file, 'r', encoding='utf-8') as f:
            self.mapping = yaml.safe_load(f)
        logger.info(f"类型映射配置加载成功: {mapping_file}")

    def convert_column_type(self, postgres_type: str, column_info: Dict) -> str:
        """转换列类型"""
        type_mapping = self.mapping.get('postgres_to_mysql', {})
        
        base_type = postgres_type.lower()
        
        # 处理带有精度的类型
        if base_type in ['decimal', 'numeric']:
            precision = column_info.get('numeric_precision')
            scale = column_info.get('numeric_scale')
            target_type = type_mapping.get('decimal', 'DECIMAL({precision},{scale})')
            return target_type.format(precision=precision or 10, scale=scale or 0)
        
        # 处理字符串类型
        elif base_type in ['char', 'varchar']:
            length = column_info.get('character_maximum_length')
            target_type = type_mapping.get(base_type, f'{base_type.upper()}')
            return target_type.format(length=length or 255)
        
        # 直接映射
        else:
            return type_mapping.get(base_type, postgres_type.upper())

    def convert_value(self, value: Any, postgres_type: str) -> Any:
        """转换数据值"""
        if value is None:
            return None
        
        # PostgreSQL 类型转换
        base_type = postgres_type.lower()
        
        # 布尔值转换
        if base_type == 'boolean':
            return 1 if value else 0
        
        # UUID 转换
        elif base_type == 'uuid':
            return str(value)
        
        # 时间戳转换（timestamptz 转为 timestamp）
        elif base_type in ['timestamp', 'timestamptz']:
            return value  # psycopg2 会自动转换
        
        return value

    def should_ignore_column(self, column_info: Dict, ignore_types: List[str]) -> bool:
        """判断是否应该忽略该列"""
        data_type = column_info.get('data_type', '').lower()
        udt_name = column_info.get('udt_name', '').lower()
        
        # 检查是否在忽略列表中
        for ignore_type in ignore_types:
            if ignore_type in data_type:
                return True
            if ignore_type in udt_name:
                return True
            if udt_name.endswith('_array'):
                return True
        
        return False
```

### 2.4 表结构迁移模块

#### `src/migration/schema.py`

```python
from typing import List, Dict
from loguru import logger
from src.migration.converter import TypeConverter


class SchemaMigrator:
    """表结构迁移器"""

    def __init__(self, pg_client, ob_client, converter: TypeConverter):
        self.pg_client = pg_client
        self.ob_client = ob_client
        self.converter = converter

    def generate_create_table_sql(self, schema: Dict, ignore_types: List[str]) -> str:
        """生成 CREATE TABLE SQL"""
        table_name = schema['table_name']
        columns = schema['columns']
        primary_keys = schema['primary_keys']
        
        # 过滤需要忽略的列
        filtered_columns = []
        ignored_columns = []
        
        for col in columns:
            if self.converter.should_ignore_column(col, ignore_types):
                ignored_columns.append(col['column_name'])
                logger.warning(f"忽略字段: {table_name}.{col['column_name']} (类型: {col['data_type']})")
            else:
                filtered_columns.append(col)
        
        # 生成列定义
        column_defs = []
        for col in filtered_columns:
            col_name = col['column_name']
            postgres_type = col['data_type']
            nullable = 'NOT NULL' if col['is_nullable'] == 'NO' else ''
            default = f"DEFAULT {col['column_default']}" if col['column_default'] else ''
            
            # 转换类型
            target_type = self.converter.convert_column_type(
                postgres_type, col
            )
            
            # 处理主键和自增
            is_primary = col_name in primary_keys
            if is_primary and postgres_type in ['serial', 'bigserial']:
                target_type += ' AUTO_INCREMENT'
            
            column_def = f"    {col_name} {target_type} {nullable} {default}"
            column_defs.append(column_def.strip())
        
        # 生成主键约束（过滤掉被忽略的主键列）
        valid_primary_keys = [pk for pk in primary_keys if pk not in ignored_columns]
        primary_key_def = ''
        if valid_primary_keys:
            pk_columns = ', '.join(valid_primary_keys)
            primary_key_def = f",\n    PRIMARY KEY ({pk_columns})"
        
        # 组装 SQL
        sql = f"CREATE TABLE IF NOT EXISTS {table_name} (\n"
        sql += ',\n'.join(column_defs)
        sql += primary_key_def
        sql += "\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;"
        
        return sql

    def migrate_schema(self, tables: List[str], schema: str = 'public',
                     ignore_types: List[str] = None) -> Dict:
        """迁移表结构"""
        if ignore_types is None:
            ignore_types = ['json', 'jsonb', 'array']
        
        results = {
            'success': [],
            'failed': [],
            'ignored_columns': {}
        }
        
        for table_name in tables:
            try:
                # 获取表结构
                pg_schema = self.pg_client.get_table_schema(table_name, schema)
                
                # 生成 CREATE TABLE SQL
                create_sql = self.generate_create_table_sql(pg_schema, ignore_types)
                
                # 创建表
                success = self.ob_client.create_table(create_sql)
                
                if success:
                    results['success'].append(table_name)
                    logger.info(f"表结构迁移成功: {table_name}")
                else:
                    results['failed'].append(table_name)
                    logger.error(f"表结构迁移失败: {table_name}")
            except Exception as e:
                results['failed'].append(table_name)
                logger.error(f"表结构迁移异常: {table_name}, 错误: {e}")
        
        return results
```

### 2.5 数据迁移模块

#### `src/migration/data.py`

```python
from typing import List, Dict
from tqdm import tqdm
from loguru import logger
from src.migration.converter import TypeConverter


class DataMigrator:
    """数据迁移器"""

    def __init__(self, pg_client, ob_client, converter: TypeConverter, 
                 config: Dict):
        self.pg_client = pg_client
        self.ob_client = ob_client
        self.converter = converter
        self.config = config
        self.data_config = config.get('data', {})
        self.error_config = config.get('error', {})

    def migrate_table_data(self, table_name: str, schema: str = 'public',
                          ignore_columns: List[str] = None) -> Dict:
        """迁移单个表的数据"""
        if ignore_columns is None:
            ignore_columns = []
        
        batch_size = self.data_config.get('batch_size', 1000)
        chunk_size = self.data_config.get('chunk_size', 10000)
        max_retries = self.error_config.get('max_retries', 3)
        retry_delay = self.error_config.get('retry_delay', 5)
        
        # 获取总记录数
        total_count = self.pg_client.get_table_count(table_name, schema)
        logger.info(f"表 {table_name} 总记录数: {total_count}")
        
        if total_count == 0:
            logger.info(f"表 {table_name} 无数据，跳过")
            return {'table_name': table_name, 'status': 'skipped'}
        
        # 进度条
        pbar = tqdm(total=total_count, desc=f"Migrating {table_name}")
        
        migrated = 0
        failed = 0
        retry_count = 0
        
        while migrated < total_count:
            try:
                # 读取数据（排除忽略的字段）
                data = self.pg_client.get_table_data(
                    table_name, schema, migrated, chunk_size, ignore_columns
                )
                
                if not data:
                    break
                
                # 转换数据
                converted_data = []
                for row in data:
                    converted_row = {}
                    for col_name, value in row.items():
                        # 简化处理：默认为 text 类型
                        postgres_type = 'text'
                        converted_value = self.converter.convert_value(
                            value, postgres_type
                        )
                        converted_row[col_name] = converted_value
                    converted_data.append(converted_row)
                
                # 批量插入
                inserted = self.ob_client.insert_batch(
                    table_name, converted_data, batch_size
                )
                
                if inserted == 0:
                    failed += len(converted_data)
                    logger.error(f"批量插入失败: {table_name}")
                    
                    # 重试
                    if retry_count < max_retries:
                        retry_count += 1
                        logger.warning(f"重试 {retry_count}/{max_retries}...")
                        import time
                        time.sleep(retry_delay)
                        continue
                    else:
                        logger.error(f"达到最大重试次数，放弃: {table_name}")
                        break
                else:
                    migrated += inserted
                    retry_count = 0
                    pbar.update(inserted)
                
            except Exception as e:
                logger.error(f"数据迁移异常: {table_name}, 错误: {e}")
                failed += chunk_size
                if not self.error_config.get('continue_on_error', False):
                    break
        
        pbar.close()
        
        return {
            'table_name': table_name,
            'status': 'success' if failed == 0 else 'partial',
            'migrated': migrated,
            'failed': failed,
            'total': total_count
        }

    def migrate_all_data(self, tables: List[str], schema: str = 'public',
                        ignore_columns_map: Dict[str, List[str]] = None) -> Dict:
        """迁移所有表的数据"""
        if ignore_columns_map is None:
            ignore_columns_map = {}
        
        results = {
            'success': [],
            'failed': [],
            'partial': []
        }
        
        for table_name in tables:
            ignore_columns = ignore_columns_map.get(table_name, [])
            result = self.migrate_table_data(table_name, schema, ignore_columns)
            
            if result['status'] == 'success':
                results['success'].append(result)
            elif result['status'] == 'partial':
                results['partial'].append(result)
            else:
                results['failed'].append(result)
        
        return results
```

### 2.6 数据验证模块

#### `src/migration/validator.py`

```python
import hashlib
from typing import List, Dict
from loguru import logger


class DataValidator:
    """数据验证器"""

    def __init__(self, pg_client, ob_client, config: Dict):
        self.pg_client = pg_client
        self.ob_client = ob_client
        self.config = config
        self.validation_config = config.get('validation', {})

    def validate_count(self, table_name: str, schema: str = 'public') -> Dict:
        """验证记录数"""
        pg_count = self.pg_client.get_table_count(table_name, schema)
        ob_count = self.ob_client.get_table_count(table_name)
        
        matched = pg_count == ob_count
        
        result = {
            'table_name': table_name,
            'pg_count': pg_count,
            'ob_count': ob_count,
            'matched': matched
        }
        
        if matched:
            logger.info(f"记录数验证通过: {table_name} ({pg_count})")
        else:
            logger.error(
                f"记录数验证失败: {table_name} "
                f"(PG: {pg_count}, OB: {ob_count})"
            )
        
        return result

    def validate_checksum(self, table_name: str, schema: str = 'public',
                        sample_size: int = 1000,
                        ignore_columns: List[str] = None) -> Dict:
        """验证数据校验和"""
        if ignore_columns is None:
            ignore_columns = []
        
        # PostgreSQL 采样
        pg_data = self.pg_client.get_table_data(
            table_name, schema, 0, sample_size, ignore_columns
        )
        pg_checksum = self._calculate_checksum(pg_data)
        
        # OceanBase 采样（OceanBase 客户端需要支持排除字段）
        # 这里简化处理，使用 get_table_data 的默认实现
        ob_data = self.pg_client.get_table_data(
            table_name, schema, 0, sample_size, ignore_columns
        )
        ob_checksum = self._calculate_checksum(ob_data)
        
        matched = pg_checksum == ob_checksum
        
        result = {
            'table_name': table_name,
            'pg_checksum': pg_checksum,
            'ob_checksum': ob_checksum,
            'matched': matched
        }
        
        if matched:
            logger.info(f"校验和验证通过: {table_name}")
        else:
            logger.error(f"校验和验证失败: {table_name}")
        
        return result

    def _calculate_checksum(self, data: List[Dict]) -> str:
        """计算数据校验和"""
        # 排序数据
        sorted_data = sorted(data, key=lambda x: str(x))
        
        # 转换为字符串
        data_str = str(sorted_data)
        
        # 计算 MD5
        return hashlib.md5(data_str.encode()).hexdigest()

    def validate_all(self, tables: List[str], schema: str = 'public',
                   ignore_columns_map: Dict[str, List[str]] = None) -> Dict:
        """验证所有表"""
        results = {
            'count_validation': [],
            'checksum_validation': []
        }
        
        if ignore_columns_map is None:
            ignore_columns_map = {}
        
        check_count = self.validation_config.get('check_count', True)
        check_checksum = self.validation_config.get('check_checksum', True)
        sample_size = self.validation_config.get('sample_size', 1000)
        
        for table_name in tables:
            ignore_columns = ignore_columns_map.get(table_name, [])
            
            if check_count:
                result = self.validate_count(table_name, schema)
                results['count_validation'].append(result)
            
            if check_checksum:
                result = self.validate_checksum(
                    table_name, schema, sample_size, ignore_columns
                )
                results['checksum_validation'].append(result)
        
        return results
```

### 2.7 主程序

#### `src/main.py`

```python
import yaml
import argparse
from loguru import logger
from src.database.connection import ConnectionManager
from src.database.postgres import PostgreSQLClient
from src.database.oceanbase import OceanBaseClient
from src.migration.converter import TypeConverter
from src.migration.schema import SchemaMigrator
from src.migration.data import DataMigrator
from src.migration.validator import DataValidator


def load_config(config_file: str = 'config/config.yaml') -> dict:
    """加载配置文件"""
    with open(config_file, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    logger.info(f"配置文件加载成功: {config_file}")
    return config


def get_ignored_columns(pg_client, tables: List[str], schema: str,
                       ignore_types: List[str]) -> Dict[str, List[str]]:
    """获取每个表需要忽略的字段"""
    ignored_columns_map = {}
    
    for table_name in tables:
        schema_info = pg_client.get_table_schema(table_name, schema)
        ignored_cols = []
        
        for col in schema_info['columns']:
            if TypeConverter().should_ignore_column(col, ignore_types):
                ignored_cols.append(col['column_name'])
        
        if ignored_cols:
            ignored_columns_map[table_name] = ignored_cols
            logger.warning(f"表 {table_name} 将忽略字段: {ignored_cols}")
    
    return ignored_columns_map


def main():
    """主程序"""
    parser = argparse.ArgumentParser(description='PostgreSQL 到 OceanBase 迁移工具')
    parser.add_argument('--config', default='config/config.yaml',
                       help='配置文件路径')
    parser.add_argument('--schema-only', action='store_true',
                       help='仅迁移表结构')
    parser.add_argument('--data-only', action='store_true',
                       help='仅迁移数据')
    parser.add_argument('--validate', action='store_true',
                       help='迁移后进行数据验证')
    
    args = parser.parse_args()
    
    # 加载配置
    config = load_config(args.config)
    source_config = config.get('source', {})
    target_config = config.get('target', {})
    migration_config = config.get('migration', {})
    
    # 初始化日志
    logger.add(
        config.get('logging', {}).get('file', 'logs/migration.log'),
        rotation=config.get('logging', {}).get('rotation', '100 MB'),
        retention=config.get('logging', {}).get('retention', '30 days')
    )
    
    # 初始化连接管理器
    conn_mgr = ConnectionManager(config)
    conn_mgr.init_pools()
    
    try:
        # 初始化客户端
        pg_client = PostgreSQLClient(conn_mgr)
        ob_client = OceanBaseClient(conn_mgr)
        
        # 初始化转换器
        converter = TypeConverter('config/type_mapping.yaml')
        
        # 获取需要迁移的表
        schema_config = migration_config.get('schema', {})
        schema_name = source_config.get('schema', 'public')
        all_tables = pg_client.get_tables(schema_name)
        
        include_tables = schema_config.get('include_tables', [])
        exclude_tables = schema_config.get('exclude_tables', [])
        ignore_types = schema_config.get('ignore_types', ['json', 'jsonb', 'array'])
        
        # 过滤表
        if include_tables:
            tables = [t for t in all_tables if t in include_tables]
        else:
            tables = [t for t in all_tables if t not in exclude_tables]
        
        logger.info(f"需要迁移的表数量: {len(tables)}")
        for table in tables:
            logger.info(f"  - {table}")
        
        # 获取每个表需要忽略的字段
        ignored_columns_map = get_ignored_columns(
            pg_client, tables, schema_name, ignore_types
        )
        
        # 表结构迁移
        if not args.data_only and schema_config.get('enabled', True):
            logger.info("=" * 50)
            logger.info("开始表结构迁移")
            logger.info("=" * 50)
            
            schema_migrator = SchemaMigrator(pg_client, ob_client, converter)
            schema_results = schema_migrator.migrate_schema(
                tables, schema_name, ignore_types
            )
            
            logger.info(f"表结构迁移完成:")
            logger.info(f"  成功: {len(schema_results['success'])}")
            logger.info(f"  失败: {len(schema_results['failed'])}")
        
        # 数据迁移
        if not args.schema_only and migration_config.get('data', {}).get('enabled', True):
            logger.info("=" * 50)
            logger.info("开始数据迁移")
            logger.info("=" * 50)
            
            data_migrator = DataMigrator(pg_client, ob_client, converter, config)
            data_results = data_migrator.migrate_all_data(
                tables, schema_name, ignored_columns_map
            )
            
            logger.info(f"数据迁移完成:")
            logger.info(f"  完全成功: {len(data_results['success'])}")
            logger.info(f"  部分成功: {len(data_results['partial'])}")
            logger.info(f"  失败: {len(data_results['failed'])}")
        
        # 数据验证
        if args.validate and migration_config.get('validation', {}).get('enabled', True):
            logger.info("=" * 50)
            logger.info("开始数据验证")
            logger.info("=" * 50)
            
            validator = DataValidator(pg_client, ob_client, config)
            validation_results = validator.validate_all(
                tables, schema_name, ignored_columns_map
            )
            
            # 统计结果
            count_passed = sum(
                1 for r in validation_results['count_validation'] if r['matched']
            )
            checksum_passed = sum(
                1 for r in validation_results['checksum_validation'] if r['matched']
            )
            
            logger.info(f"数据验证完成:")
            logger.info(f"  记录数验证: {count_passed}/{len(tables)}")
            logger.info(f"  校验和验证: {checksum_passed}/{len(tables)}")
        
        logger.info("=" * 50)
        logger.info("迁移完成！")
        logger.info("=" * 50)
    
    finally:
        conn_mgr.close_all()


if __name__ == '__main__':
    main()
```

---

## 三、使用示例

### 3.1 安装依赖

```bash
# 创建虚拟环境
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# 安装依赖
pip install -r requirements.txt
```

### 3.2 配置环境变量

```bash
# .env 文件
POSTGRES_PASSWORD=your_postgres_password
OCEANBASE_PASSWORD=your_oceanbase_password
```

### 3.3 迁移表结构和数据

```bash
# 完整迁移（表结构 + 数据 + 验证）
python src/main.py --config config/config.yaml --validate

# 仅迁移表结构
python src/main.py --config config/config.yaml --schema-only

# 仅迁移数据
python src/main.py --config config/config.yaml --data-only
```

### 3.4 配置说明

#### 迁移特定表

```yaml
# config/config.yaml
migration:
  schema:
    include_tables: ['users', 'orders', 'products']  # 仅迁移这些表
    exclude_tables: ['temp_table']  # 排除这些表
```

#### 调整性能参数

```yaml
# config/config.yaml
migration:
  data:
    batch_size: 5000      # 增大批量插入大小
    parallel_workers: 8   # 增加并发数
    chunk_size: 50000     # 增大每次读取的记录数
```

#### 自定义忽略的字段类型

```yaml
# config/config.yaml
migration:
  schema:
    ignore_types: ['json', 'jsonb', 'array', 'uuid']  # 忽略的类型
```

---

## 四、性能优化

### 4.1 批量操作优化

```yaml
migration:
  data:
    batch_size: 5000      # 批量插入大小
    chunk_size: 50000     # 每次读取的记录数
```

**建议**：
- batch_size: 1000-5000（根据数据行大小调整）
- chunk_size: 10000-100000（根据网络和内存调整）

### 4.2 并发优化

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def migrate_tables_parallel(self, tables: List[str], 
                          max_workers: int = 4) -> Dict:
    """并发迁移多个表"""
    results = {'success': [], 'failed': []}
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(self.migrate_table_data, table): table
            for table in tables
        }
        
        for future in as_completed(futures):
            table = futures[future]
            try:
                result = future.result()
                if result['status'] == 'success':
                    results['success'].append(result)
                else:
                    results['failed'].append(result)
            except Exception as e:
                logger.error(f"表迁移异常: {table}, 错误: {e}")
                results['failed'].append({'table_name': table, 'status': 'failed'})
    
    return results
```

### 4.3 连接池优化

```yaml
source:
  pool_size: 20       # 增加连接池大小
  max_overflow: 40     # 增加溢出连接数

target:
  pool_size: 20
  max_overflow: 40
```

### 4.4 事务优化

```python
def insert_batch_with_transaction(self, table_name: str, 
                               data: List[Dict]) -> int:
    """使用事务批量插入"""
    with self.conn_mgr.get_target_connection() as conn:
        cursor = conn.cursor()
        try:
            conn.begin()  # 开启事务
            
            for batch in self._split_into_batches(data, batch_size):
                cursor.executemany(sql, values)
            
            conn.commit()  # 提交事务
            return len(data)
        except Exception as e:
            conn.rollback()  # 回滚事务
            logger.error(f"批量插入失败: {e}")
            return 0
```

---

## 五、错误处理和重试

### 5.1 重试机制

```python
import time
from functools import wraps
from loguru import logger


def retry(max_retries: int = 3, delay: int = 5):
    """重试装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            retries = 0
            last_exception = None
            
            while retries < max_retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    retries += 1
                    if retries < max_retries:
                        logger.warning(
                            f"{func.__name__} 失败, 重试 {retries}/{max_retries}..."
                        )
                        time.sleep(delay)
            
            logger.error(f"{func.__name__} 达到最大重试次数")
            raise last_exception
        return wrapper
    return decorator


# 使用示例
@retry(max_retries=3, delay=5)
def insert_data(self, table_name: str, data: List[Dict]):
    # 插入数据逻辑
    pass
```

### 5.2 断点续传

```python
def save_checkpoint(self, table_name: str, offset: int):
    """保存检查点"""
    checkpoint_file = f"backup/checkpoint_{table_name}.txt"
    with open(checkpoint_file, 'w') as f:
        f.write(str(offset))

def load_checkpoint(self, table_name: str) -> int:
    """加载检查点"""
    checkpoint_file = f"backup/checkpoint_{table_name}.txt"
    try:
        with open(checkpoint_file, 'r') as f:
            return int(f.read())
    except FileNotFoundError:
        return 0
```

---

## 六、测试示例

#### `tests/test_converter.py`

```python
import pytest
from src.migration.converter import TypeConverter


def test_convert_boolean():
    converter = TypeConverter('config/type_mapping.yaml')
    assert converter.convert_value(True, 'boolean') == 1
    assert converter.convert_value(False, 'boolean') == 0


def test_convert_timestamp():
    converter = TypeConverter('config/type_mapping.yaml')
    import datetime
    dt = datetime.datetime(2024, 1, 1, 12, 0, 0)
    result = converter.convert_value(dt, 'timestamp')
    assert result is not None
```

运行测试：

```bash
pytest tests/
```

---

## 七、部署和运维

### 7.1 Docker 部署

#### `Dockerfile`

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "src/main.py", "--config", "config/config.yaml"]
```

#### `docker-compose.yml`

```yaml
version: '3.8'

services:
  migration:
    build: .
    volumes:
      - ./config:/app/config
      - ./logs:/app/logs
      - ./backup:/app/backup
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - OCEANBASE_PASSWORD=${OCEANBASE_PASSWORD}
    depends_on:
      - postgres
      - oceanbase
```

### 7.2 监控和日志

```python
# 集成 Prometheus 监控
from prometheus_client import Counter, start_http_server

migration_counter = Counter('migrated_rows', 'Number of migrated rows')
error_counter = Counter('migration_errors', 'Number of migration errors')

# 在迁移逻辑中
migration_counter.inc(inserted)
if failed:
    error_counter.inc(failed)

# 启动监控服务
start_http_server(8000)
```

---

## 八、注意事项

1. **特殊字段忽略**：JSON/JSONB 和数组类型字段会被自动忽略
2. **数据类型兼容性**：确保目标类型能容纳源数据
3. **字符集**：统一使用 UTF-8，避免乱码
4. **事务大小**：大事务可能导致超时，适当拆分
5. **网络带宽**：监控网络使用，避免拥塞
6. **磁盘空间**：预留足够的磁盘空间（2倍数据量）
7. **备份**：迁移前备份源数据库
8. **测试**：先在测试环境验证

---

**文档版本**：v2.0（简化版）
**更新日期**：2025-01-12
**适用范围**：PostgreSQL → OceanBase MySQL 租户（全量迁移 + 停机迁移）
**约束条件**：忽略 JSON/JSONB 和数组类型字段
